{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Midterm\n",
    "Machine Learning for the Social Sciences\n",
    "Prof. Michael Parrot\n",
    "17 December 2021\n",
    "Yun Choi"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n",
    "\n",
    "Regression models, such as linear, logistic, ridge and lasso, allow measuring the sole effect of an independent variable on the dependent variable by separating the effects of other explanatory variables. If standardized, a large absolute coefficient indicates that the variable is an important predictor of the dependent variable.\n",
    "Compared to classical regression models, penalized ones primarily used for predictions in machine learning, are less useful for social scientists whose main purpose is to explain social phenomena. Ridge and lasso shrink the coefficients to lower the model complexity and achieve high predictive power. However, high predictive power comes at a cost. Lowering the model complexity means a worse fit to the existing data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Describe the main differences between supervised and unsupervised learning.\n",
    "\n",
    "The main difference between supervised and unsupervised learning is the use of labeled datasets. Supervised learning uses labeled data, while unsupervised learning does not.\n",
    "Supervised learning aims to learn labeled data and make predictions when given predictor values. Supervised learning models are categorized into classifiers and regressors based on what kind of predictions they make. Models predicting a discrete class label are classifiers. Ones predicting a continuous quantity are regressors.\n",
    "On the other hand, the primary purpose of unsupervised learning is to further understand the data rather than predict. Without any labeled dataset, a model cannot be evaluated for its predictive power."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners? For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
    "\n",
    "Supervised learning is the primary approach being used by machine learning practitioners due to its measurability and predictability. With a labeled dataset, a modeler can measure the success of the model they created. Models with high cross-validation and test scores are expected to make predictions with a predictable accuracy given new data.\n",
    "On the other hand, unsupervised learning models cannot be measured for their quality due to a lack of labeled data. Therefore, their performances are unpredictable, which is the characteristic that prohibits the models from being widely accepted by the industry.\n",
    "Nonetheless, unsupervised learning can be used to understand unlabeled data further or reduce the dimension of labeled data at the preprocessing stage before building a supervised learning model. Unsupervised learning models primarily perform three tasks: clustering, association, and dimensionality reduction. Clustering groups similar observations; association finds relationships between variables in a given dataset; and dimensionality reduction reduces the number of features in the dataset.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?\n",
    "\n",
    "As mentioned above, unsupervised learning is primarily for three tasks: clustering, association, and dimensionality reduction. This semester, we have learned several approaches for clustering and dimensionality reduction.\n",
    "For clustering, we learned k-means clustering, and hierarchical clustering. K-means clustering groups observations into a pre-specified number of clusters in a way that minimizes within-cluster variance. On the other hand, hierarchical clustering builds a hierarchy of clusters without having a fixed number of clusters. Therefore, K-means clustering is less preferable when the modeler does not have any prior knowledge of the number of clusters.\n",
    "For dimensionality reduction, we have learned Principal Component Analysis (PCA), and manifold learning. PCA is the most commonly used approach for dimensionality reduction because it significantly reduces the number of features in the dataset while maintaining the essential relationships between the points. However, PCA, which creates several linear hyperplanes to represent dimensions, is less effective with data that have a non-linear structure. Manifold learning is more suitable for such data because it better fits non-linearly structured data by rotating, scaling or re-orienting the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. What are the main benefits of using Principal Components Analysis?\n",
    "\n",
    "PCA is primarily used for dimensionality reduction. It reduces the number of features in the dataset while maintaining essential relationships among data points. Low-dimensional data produced by PCA is better to work with because it requires lower storage and computing power, and is easier to visualize than high-dimensional data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n",
    "\n",
    "Convolutional neural networks (CNNs) work better than multilayer perceptron networks (MLPs) in image processing because of the three following differences.\n",
    "First, MLPs use one pixel for each input, unlike CNNs, in which the unit of inputs can be larger and specified by modelers. Using one pixel for each input creates too many parameters, requiring large memory and slowing down the computation.\n",
    "Second, MLPs take vectors as inputs, while CNNs can also take matrices. Therefore, an image should be flattened to be processed in MLPs. However, spatial information, such as correlation with neighboring pixels, is lost in the flattening process. The implication of losing spatial information is that the model reacts differently to an input and its shifted version. In machine learning parlance, it becomes “variant to translation.” On the other hand, CNNs can maintain spatial information and become invariant to translation.\n",
    "Third, MLPs have fully connected layers only, while CNNs can have other types of layers like convolutional or pooling layers. In a fully connected layer, each unit is connected with each unit in the previous layer. However, in a convolutional layer, each unit does not have to be connected to each unit in the previous layer, which gives the model more flexibility in learning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers. 50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories. Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(50, activation = 'relu', input_dim=input_dim))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(150, activation = 'relu'))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(75, activation='relu', input_dim=input_dim))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers. 16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(16, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(28, kernel_size=kernel_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Write the keras code for a convolutional neural network with the following structure: Two convolutional layers. 32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(32, kernel_size=kernel_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer ='sgd', metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "aimodelshare username: choiyun0830"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}